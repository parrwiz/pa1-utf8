1:  UTF-32 uses a fixed 4 bytes per code point, making it simple to index and manipulate - just multiply the index by 4. However, this simplicity comes at a storage cost. For English text (ASCII), UTF-32 uses 4x more space than UTF-8. UTF-8 is more space-efficient for ASCII-heavy text, using just 1 byte for ASCII characters, but requires more complex processing to find the nth character or to parse. For example, to find the 5th character in UTF-8, you need to parse the string from the beginning, while in UTF-32 you can jump directly to byte 20 (5 Ã— 4).

2:  The leading '10' in continuation bytes serves as a synchronization mechanism. If a program starts reading in the middle of a UTF-8 stream (like when seeking in a file), it can easily find the start of the next valid code point by skipping bytes beginning with '10' until it finds a start byte. Without this, a program might misinterpret continuation bytes as start bytes, leading to data corruption or display errors.
